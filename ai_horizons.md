
# Hurdles to AI

At the time of writing this, I see three main limitations of the evolution of AI:
* Deep learning
* Coding
* Digital Computers

The above follows the "bad to worse" type of joke structure. 

The punchline is I'm not kidding. 

Now, of course, I wouldn't be writing this, or have the wonderful job I have, if it weren't for digital computers and the languages to talk to them. 
And deep learning, the (current) cherry on top of the machine learning cake: They can do magic.

But we can do better. Of course.

So let's have a look at what we could do differently.

## Deep learning
Deep learning is greedy, brittle, opaque, and shallow, says Gary Marcus (professor of cognitive psychology at NYU). 

Think outside the hypercube!

We're trying to model the mind by modeling the brain, and modeling the brain with structured layers.
The mind has more of the messy multi-connected structure of language than the neat arrangements of matrices.

## Coding
I said "coding" instead of "programming languages" on purpose. It's the "programming" that I'm pointing my accusatory finger at, 
not the language part. 
Again, language is close to the mind, and will be involved in our communicating our designs to a computer. 
But does that communication have to be so darn unforgivingly dry and fragile?

The way we talk to computers still carries a heavy "engineering" legacy; switches to flip, wires to connect, etc.
But that's not the way we think, so the idea-to-implementation path is slow and creativity is limited.

## Digital Computers
Intelligence is not binary.

Reality is not precise.

Harness uncertainty instead of fighting it!


# Guide the natural semantic emergence

_"(â€¦) deep learning dynamics can self-organize emergent hidden representations in a manner that recapitulates many empirical phenomena in human development."_ [A mathematical theory of semantic development in deep neural networks](https://www.pnas.org/content/116/23/11537): 

Yet, I still insist that the industry should spend more effort modeling the mind with language-like structures and is spending too much on modeling the brain with rectangular structures.

Let's assume, as the article claims, that language-like organization naturally emerges from the rectangular ones.My guess is that there's many ways to do this, and therefore it will be improbable that the internal representations the machine will develop will be compatible with ours.

Perhaps machines don't need any guidance at all, but if we want the human+machine system to be optimal for humans, we may want to provide this guidance anyway.

One way to do this is to constrain the space of possibilities to resemble to something resembling our minds (rather than our brains).

